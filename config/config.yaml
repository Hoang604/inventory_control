env:
  state_dim: 33
  action_dim: 3
intermediate_dim: 1024
actor:
  log_std_max: 2
  log_std_min: -20
  output_dim: 6
iql:
  learning_rate: 1e-4
  actor_learning_rate: 1e-5
  eta_min: 1e-7
  tau: 0.8
  gamma: 0.95
  alpha: 0.005
  beta: 1.0
environment:
  num_warehouses: 10
  days_per_warehouse: 3650
dataset:
  # Available: "continuing", "base_stock", "single_expert", "multi_policy"
  name: "multi_policy"
  # Optional: override with explicit path (if set, 'name' is ignored)
  # path: "data/custom_dataset.pt"
training:
  batch_size: 512
  epochs: 100
  reward_scale: 0.1
  validation_split: 0.8
  seed: 42
  grad_norm_clip: 1.0
  adv_weight_clip: 100.0
  log_interval: 1
  checkpoint_interval: 5
