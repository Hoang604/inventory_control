\begin{abstract}
Reinforcement Learning (RL) promises to revolutionize supply chain management by optimizing for long-term value in stochastic environments. However, industrial adoption is stalled by the ``Exploration Tax''---the prohibitive cost of online trial-and-error. Offline RL offers a compelling alternative: learning policies entirely from historical logs. A critical open question is whether offline agents can surpass the performance of the high-quality heuristics (e.g., Base-Stock policies) that generated their data. In this work, we present a successful application of Implicit Q-Learning (IQL) to multi-echelon inventory control, training on a dataset composed of diverse, expert-level Base-Stock policies. We demonstrate that IQL does not merely imitate these 5 experts; by leveraging expectile regression ($\tau=0.7$) as a value-maximization filter, it synthesizes a control strategy that outperforms the best heuristic in the dataset. Our experiments on the \texttt{InvManagement-v1} environment reveal that the IQL agent achieves a 17.41\% increase in mean profit over the optimized Base-Stock baseline ($p < 10^{-73}$). This result challenges the assumption that Offline RL is limited to behavior cloning, proving that it can effectively transcend the performance ceiling of the experts provided in the static dataset.
\end{abstract}
