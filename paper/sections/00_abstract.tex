\begin{abstract}
Reinforcement Learning (RL) holds immense promise for optimizing complex supply chains, yet its industrial adoption is paralyzed by the ``Exploration Tax''â€”the prohibitive cost of learning through trial-and-error in the real world. While standard RL requires risky online interaction, Offline RL offers a path to learn policies entirely from static historical logs. However, a key challenge remains: how to extract optimal behavior from datasets that are dominated by suboptimal, noisy, or chaotic heuristics. In this work, we present a successful application of Implicit Q-Learning (IQL) to multi-echelon inventory management. We demonstrate that by leveraging expectile regression, IQL acts as a mathematical sieve, filtering out the ``bad variance'' of randomized Min-Max heuristics to distill a stable, high-performance control strategy. Our experiments on the \texttt{InvManagement-v1} environment show that the offline agent does not merely improve average profit by 96\%; more importantly, it collapses the performance variance by over 90\%. This result challenges the prevailing assumption that high-quality data is a prerequisite for offline learning, proving that autonomous agents can mine mastery from the noise of suboptimal history.
\end{abstract}