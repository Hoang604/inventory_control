Supply Chain Management (SCM) is defined by the tension between efficiency and volatility. The modern inventory manager operates in a hostile environment of fluctuating demand, variable lead times, and non-linear costs. Traditional control heuristics, such as Base-Stock or Min-Max $(s, S)$ policies, attempt to tame this chaos with rigid rules. However, these static heuristics are brittle; they require precise parameter tuning and often fail catastrophically when environmental conditions drift outside their designed envelopes \cite{silver2016inventory}.

Reinforcement Learning (RL) offers a compelling alternative: an adaptive agent that learns to navigate the chaos by optimizing for long-term value rather than adhering to fixed thresholds. Yet, despite years of academic success \cite{hubbs2020orgym}, RL remains largely absent from real-world logistics. The barrier is not algorithmic but economic. Standard ``Online'' RL algorithms learn by exploring—taking random actions to discover their consequences. In robotics, a failed exploration step is a reset; in a supply chain, it is a stockout of critical medicine, a halted production line, or a warehouse overflow. The \textbf{Exploration Tax}—the cost of learning by failing—is simply too high for industrial systems to pay.

Offline Reinforcement Learning (Batch RL) fundamentally alters this value proposition. It promises to learn effective policies entirely from static, historical datasets, without a single moment of risky online interaction \cite{levine2020offline}. This shifts the problem from ``how do we safely explore?'' to ``how do we mine wisdom from our history?''

However, this shift introduces the \textbf{Paradox of Static Mastery}: How can an agent that never interacts with the world outperform the very policies that generated its data? This is particularly challenging in SCM, where historical logs are often generated by suboptimal, legacy heuristics that are riddled with noise and inefficiency.

In this paper, we address this paradox by applying Implicit Q-Learning (IQL) \cite{kostrikov2021iql} to the domain of multi-echelon inventory control. We construct a dataset not from expert demonstrations, but from a chaotic mix of randomized Min-Max policies, simulating a history of suboptimal decision-making. We argue that IQL's specific mechanism—expectile regression—serves as a robust filter for this chaos. Unlike standard methods that average out performance or overestimate outliers, IQL learns to selectively imitate the ``lucky'' moments of the heuristics while discarding their failures. The result is an agent that is not only more profitable but, crucially, radically more stable than the baseline.