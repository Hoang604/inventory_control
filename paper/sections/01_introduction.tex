Supply Chain Management (SCM) is defined by the tension between efficiency and volatility. The modern inventory manager operates in a hostile environment of fluctuating demand, variable lead times, and non-linear costs. Traditional control heuristics, such as Base-Stock or Min-Max $(s, S)$ policies, attempt to tame this chaos with rigid rules. While effective, these static heuristics are fundamentally limited by their linear structure; they require precise, brittle parameter tuning and struggle to adapt to complex, non-linear state dynamics.

Reinforcement Learning (RL) offers a compelling alternative: an adaptive agent that navigates volatility by optimizing for long-term value rather than adhering to fixed thresholds. Yet, despite academic success \cite{hubbs2020orgym}, RL remains largely absent from real-world logistics. The barrier is the \textbf{Exploration Tax}â€”the cost of learning by failing. In a supply chain, a failed exploration step is a stockout of critical medicine or a halted production line.

Offline Reinforcement Learning (Batch RL) fundamentally alters this value proposition. It promises to learn effective policies entirely from static, historical datasets, without a single moment of risky online interaction \cite{levine2020offline}.

However, this paradigm shift introduces the \textbf{Paradox of Static Mastery}: Can an agent that never interacts with the world outperform the very experts that generated its data? This is particularly challenging in SCM, where historical logs are often generated by highly optimized, domain-specific heuristics.

In this paper, we address this paradox by applying Implicit Q-Learning (IQL) \cite{kostrikov2021iql} to the domain of multi-echelon inventory control. Unlike prior works that focus on recovering from random/suboptimal data, we train on a \textbf{``Mixture of Experts''} dataset generated by diverse, high-performing Base-Stock policies. We demonstrate that IQL's expectile regression mechanism allows the agent to essentially ``cherry-pick'' the optimal decisions across different experts, synthesizing a policy that is superior to any individual expert in the mixture. The resulting agent demonstrates a \textbf{20.1\% improvement} over the best-in-class Base-Stock policy, effectively breaking the ``Heuristic Ceiling'' of the dataset.