   Supply Chain Management (SCM) is defined by the fundamental tension between operational efficiency and environmental volatility. The modern inventory manager operates in a
   hostile landscape of fluctuating demand, variable lead times, and non-linear costs---a complexity that has historically been managed through rigid, linear heuristics such as t
   Base-Stock or Min-Max $(s, S)$ policies \cite{Scarf1960Optimality, Zipkin2000}. While these mechanisms are provably optimal under idealized, i.i.d. demand assumptions
   \cite{ClarkScarf1960}, their real-world performance is often undermined by the ``bullwhip effect'' \cite{Lee1997Bullwhip}, where minor demand fluctuations are amplified into
   severe oscillations upstream, driving excessive holding costs and operational instability.

   Deep Reinforcement Learning (DRL) has emerged as a promising alternative, offering adaptive agents that navigate volatility by optimizing for long-term value rather than
   adhering to fixed thresholds \cite{Sutton2018}. Recent advances have demonstrated that DRL can significantly outperform static heuristics in simulated environments
   \cite{hubbs2020orgym, Gijsbrechts2022DeepRL, Oroojlooyjadid2022DeepQN}. \textbf{However}, industrial adoption remains stalled by the \textbf{``Exploration Tax''}---the
   prohibitive cost of online trial-and-error. In a physical supply chain, a failed exploration step is not merely a numerical loss but a stockout of critical goods or a halted
   production line. Furthermore, most existing RL applications in SCM rely on the ``Sim2Real'' assumption, requiring high-fidelity digital twins that are often prohibitively
   expensive to construct and maintain \cite{Gijsbrechts2022DeepRL}.

   Offline Reinforcement Learning (Batch RL) fundamentally alters this value proposition by learning effective policies entirely from static, historical logs
   \cite{levine2020offline}. Yet, this shift introduces the \textbf{``Paradox of Static Mastery''}: Can an agent that never interacts with the world outperform the very experts
   that generated its data? This is particularly challenging in mature supply chains, where historical logs are generated by highly optimized, domain-specific heuristics.

      In this paper, we address this paradox by \textbf{formulating} the multi-echelon control problem through the lens of Implicit Q-Learning (IQL) \cite{kostrikov2021iql}. Unlike
      prior works that focus on recovering from random or suboptimal data, we train on a ``Mixture of Experts'' dataset generated by diverse, high-performing Base-Stock policies. We
      \textbf{demonstrate} that IQL's expectile regression mechanism acts as a mathematical filter, allowing the agent to \textbf{synthesize} a policy that ``cherry-picks'' the
      optimal decisions across different experts in different states. Our experiments on the \texttt{InvManagement-v1} environment reveal that the IQL agent achieves a \textbf{17.41
      increase in mean profit} over the true global optimum of the Base-Stock class ($p < 10^{-73}$), effectively breaking the ``Heuristic Ceiling'' of the dataset.
   
      \begin{figure}[ht]
          \centering
          \includegraphics[width=\linewidth]{figures/figure1_concept.pdf}
          \caption{Conceptual overview of the \textbf{Implicit Synthesis} mechanism. (A) The dataset contains a mixture of diverse expert heuristics. (B) IQL uses expectile regression ($\tau=0.7$) to estimate the upper-bound value $V(s)$, creating an ``Advantage Zone'' that filters for high-performance decisions. (C) The resulting policy synthesizes a control strategy by selectively imitating only the best decisions from the expert mixture.}
          \label{fig:concept}
      \end{figure}
   