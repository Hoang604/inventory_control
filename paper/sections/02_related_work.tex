\subsection{The Limits of Heuristics}
Inventory theory has historically been dominated by linear heuristic policies, such as the $(s, S)$ rule or Base-Stock policies. While these mechanisms are provably optimal under strict, idealized assumptions (e.g., fixed costs, i.i.d. demand), their deployment in real-world environments is contingent on brittle parameter tuning. Even when calibrated by experts, these policies remain static; they lack the capacity to **dynamically adapt** to non-stationary demand signals or complex, multi-echelon lead-time interactions. As our experiments demonstrate, even the global optimum of the parameter space is a compromise that leaves significant value on the table.

\subsection{The Cost Barrier of Online RL}
The application of Deep Reinforcement Learning (DRL) to inventory management has been extensively explored within simulated environments \cite{hubbs2020orgym}. Approaches leveraging DQN and PPO have demonstrated the capacity to outperform static heuristics by adapting to complex system dynamics. **However**, these contributions often rest on the ``Sim2Real'' assumptionâ€”the existence of a high-fidelity digital twin that perfectly mirrors the stochastic properties of the physical supply chain. In practice, such oracles are prohibitively expensive to construct and maintain. For the vast majority of industrial operations, the only available ground truth is the static log of historical transactions, rendering online exploration infeasible.

\subsection{Offline RL: The Industrial Path}
Offline RL circumvents the need for simulation by learning directly from fixed datasets. **However**, standard off-policy algorithms (e.g., DQN) catastrophically fail in this setting due to the ``Winner's Curse'': the tendency to overestimate the value of out-of-distribution actions, leading to policy collapse. While Conservative Q-Learning (CQL) addresses this by penalizing unseen actions, it often results in overly risk-averse behavior that fails to outperform the behavior policy.

Implicit Q-Learning (IQL) \cite{kostrikov2021iql} represents a fundamental paradigm shift. Rather than constraining the policy to the dataset's support, IQL treats value estimation as a supervised expectile regression problem. By regressing on the upper expectiles of the value distribution, IQL effectively asks: ``What is the best outcome we have witnessed in a similar state?'' and **synthesizes** a policy to reproduce that specific upper-bound behavior. We posit that this mechanism is uniquely suited for SCM, where the objective is to **distill** and stabilize the rare, high-performance decisions buried within the historical log.
