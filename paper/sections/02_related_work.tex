\subsection{The Limits of Heuristics}
Inventory theory has historically been dominated by linear heuristic policies, such as the $(s, S)$ rule or Base-Stock policies \cite{Scarf1960Optimality, Zipkin2000}. While these mechanisms are provably optimal under strict, idealized assumptions such as i.i.d. demand and fixed lead times \cite{ClarkScarf1960, Scarf1960Optimality}, their deployment in real-world environments is contingent on brittle parameter tuning. Even when calibrated by experts, these policies remain static; they lack the capacity to \textbf{dynamically adapt} to non-stationary demand signals or complex, multi-echelon lead-time interactions \cite{Graves1989MultiEchelon, Lee1997Bullwhip}. As our experiments demonstrate, even the global optimum of the parameter space is a compromise that leaves significant value on the table.

\subsection{The Cost Barrier of Online RL}
The application of Deep Reinforcement Learning (DRL) to inventory management has been extensively explored within simulated environments \cite{Sutton2018, hubbs2020orgym}. Approaches leveraging DQN and PPO have demonstrated the capacity to outperform static heuristics by adapting to complex system dynamics \cite{Gijsbrechts2022DeepRL, Oroojlooyjadid2022DeepQN, Leluc2023MARLIM}. \textbf{However}, these contributions often rest on the ``Sim2Real'' assumption---the existence of a high-fidelity digital twin that perfectly mirrors the stochastic properties of the physical supply chain. In practice, such oracles are prohibitively expensive to construct and maintain \cite{Gijsbrechts2022DeepRL}. For the vast majority of industrial operations, the only available ground truth is the static log of historical transactions, rendering online exploration infeasible.

\subsection{Offline RL: The Industrial Path}
Offline RL circumvents the need for simulation by learning directly from fixed datasets \cite{levine2020offline}. \textbf{However}, standard off-policy algorithms (e.g., DQN) catastrophically fail in this setting due to the ``Winner's Curse'': the tendency to overestimate the value of out-of-distribution actions, leading to policy collapse \cite{fujimoto2019off}. While Conservative Q-Learning (CQL) addresses this by penalizing unseen actions \cite{Kumar2020ConservativeQL}, it often results in overly risk-averse behavior that fails to outperform the behavior policy. Other approaches, such as TD3+BC \cite{fujimoto2021minimalist} and AWAC \cite{nair2020awac}, attempt to balance imitation and optimization, but can struggle with the high-variance returns common in supply chains.

Implicit Q-Learning (IQL) \cite{kostrikov2021iql} represents a fundamental paradigm shift. Rather than constraining the policy to the dataset's support, IQL treats value estimation as a supervised expectile regression problem \cite{NeweyPowell1987}. By regressing on the upper expectiles of the value distribution, IQL effectively asks: ``What is the best outcome we have witnessed in a similar state?'' and \textbf{synthesizes} a policy to reproduce that specific upper-bound behavior. We posit that this mechanism is uniquely suited for SCM, where the objective is to \textbf{distill} and stabilize the rare, high-performance decisions buried within the historical log, a challenge also explored in recent works on sequence modeling for RL \cite{Chen2021DecisionTransformer} and value-based regularization \cite{Brandfonbrener2021OfflineRL}. Recent studies in 2024 and 2025 have further validated the potential of DRL in retail and omni-channel environments \cite{Park2025, MaDing2025, Kaynov2024, YavuzKaya2024}, yet the specific challenge of super-human synthesis from expert mixtures remains an open frontier.
