We evaluated the IQL agent against the randomized Min-Max baseline over 50 unseen test episodes. The results, summarized in Table \ref{tab:results}, reveal a fundamental transformation in performance.

\begin{table}[ht]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Policy} & \textbf{Mean Reward (Profit)} & \textbf{Std. Dev. (Risk)} \\
        \midrule
        Randomized Min-Max (Baseline) & 42.51 & 131.17 \\
        \textbf{IQL Agent (Ours)} & \textbf{194.35} & \textbf{7.48} \\
        \bottomrule
    \end{tabular}
    \caption{Performance comparison. Note the massive collapse in Standard Deviation.}
    \label{tab:results}
\end{table}

\subsection{Stability is Profit}
While the \textbf{357\% increase in Mean Reward} (from 42.51 to 194.35) is significant, the most profound result is the \textbf{94.3\% reduction in Standard Deviation} (from 131.17 to 7.48).

In logistics, variance is a direct proxy for risk. The high variance of the baseline (131.17) reflects the fragility of heuristic policies: if the parameters (s, S) do not align with the demand wave, the supply chain either stocks out or overflows. The baseline essentially "gambles" on the parameters.

The IQL agent, conversely, has learned to stop gambling. By distilling the optimal decisions from the chaotic history, it has converged on a policy that is invariant to the parameter noise that plagued the dataset. It achieves consistent, high-level performance regardless of the initial conditions. This proves that the offline agent successfully identified the underlying structural dynamics of the environment (e.g., the 10-day lead time delay) and learned to buffer against them, rather than merely memorizing the heuristic rules.

\subsection{Mining Wisdom from Noise}
These results validate the "Paradox of Static Mastery." The IQL agent never interacted with the environment. It only saw a history of 2,000 episodes, most of which were executed by suboptimal policies. Yet, by filtering this history through the lens of expectile regression, it constructed a policy superior to any single heuristic in the dataset. This confirms that "data quality" in Offline RL is not about having expert demonstrations; it is about having \textit{diverse} demonstrations from which an expert can be synthesized.
