This study dismantles the myth that high-performance inventory control requires either perfect expert heuristics or risky online exploration. We have demonstrated that Offline Reinforcement Learning, specifically Implicit Q-Learning, can synthesize a "Super-Expert" policy from a dataset of "Mediocre" history.

Our IQL agent achieved a 357\% improvement in profit and a 10x reduction in operational variance compared to the heuristics that generated its training data. This finding has immediate industrial relevance. It implies that the terabytes of "suboptimal" historical logs currently sitting in corporate databases are not waste; they are latent gold mines of optimal control. We have shown that with the right mathematical filter, we can distill the signal from this noise, enabling the deployment of autonomous, risk-aware supply chain agents without ever paying the Exploration Tax.