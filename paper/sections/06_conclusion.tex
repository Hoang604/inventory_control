This study dismantles the limitation that Offline RL is merely a technique for recovering from failure. We have demonstrated that Implicit Q-Learning can act as a mechanism for \textbf{Super-Human Synthesis}, aggregating the partial wisdom of multiple experts to construct a policy superior to any individual teacher.

Our IQL agent achieved a \textbf{20.2\% improvement in profit} compared to the optimized Base-Stock policy that served as its best training example. This result is statistically significant ($p \approx 1.9 \times 10^{-88}$). Crucially, this performance gain was achieved without any online interaction or fine-tuning. This finding implies that historical data from diverse expert strategies—common in mature supply chains—can be leveraged not just to automate existing processes, but to break through the performance ceilings inherent in linear heuristic controls. We have shown that with the right mathematical filter ($\tau=0.8$), we can distill a non-linear, adaptive policy that transcends the experts it learned from.